{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6ace087-7f7d-472b-95de-f06b949d6ccb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8426ed77-d902-4aa6-8c78-9c1ca5a47707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from torchvision import datasets, transforms, models\n",
    "import gc\n",
    "import random\n",
    "import optuna\n",
    "import mlflow\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "967d0281-9ecc-4665-9bd2-7b81e6464e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 200\n",
    "OPT_EPOCHS = 20\n",
    "OPT_STEPS = 50\n",
    "NUM_CLASSES = 3\n",
    "BITS = 12\n",
    "IMAGE_PER_SUBJECT = 500\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "subject_file = 'subjects.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e0afc-dbbe-4317-ab43-eeddbbc5c644",
   "metadata": {},
   "source": [
    "<font size=\"5\">Custom dicom dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a347c6-f659-41ff-bcb3-ad9d205fb5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DicomData(Dataset):\n",
    "    def __init__(self, path_list, transform=None):\n",
    "        self.path_list = path_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.path_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.load_dicom_image(self.path_list[index][0])\n",
    "        label = self.path_list[index][1]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def load_dicom_image(self, path):\n",
    "        return np.float32(pydicom.dcmread(path).pixel_array)\n",
    "\n",
    "def get_image_list(root, path, label):\n",
    "    paths = pydicom.data.data_manager.get_files(os.path.join(root, path), '*.dcm')\n",
    "    random.shuffle(paths)\n",
    "    valid = []\n",
    "    for path in paths:\n",
    "        img = pydicom.dcmread(path)\n",
    "        # Discard corrupted images when listing them.\n",
    "        # It could also be done by defining custom collate_fn function for dataloader.\n",
    "        # It's better to discard them before starting the training for better training performance\n",
    "        if \"ImageType\" in img and not np.all(img.pixel_array == 0) and len(img.pixel_array.shape) == 2:\n",
    "            valid.append((path, label))\n",
    "            if len(valid) == IMAGE_PER_SUBJECT:\n",
    "                break\n",
    "    print('total: ' + str(len(paths)) + ' chosen: ' +  str(len(valid)))\n",
    "    return valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab96ae8-b6fc-4c45-a81c-1fef25eae83a",
   "metadata": {},
   "source": [
    "<font size=\"5\">Data Preparation</font>\n",
    "\n",
    "Name of the subjects, path to their data folder and their labels should be in subjects.csv.\n",
    "<br>\n",
    "Images are resized to (512, 512) and normalized based on their depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d32b14f-8d29-4941-b383-b068aa780687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 771 chosen: 500\n",
      "total: 3037 chosen: 500\n",
      "total: 3044 chosen: 500\n",
      "total: 3048 chosen: 500\n",
      "total: 3039 chosen: 500\n",
      "total: 3746 chosen: 500\n",
      "total: 2359 chosen: 500\n",
      "total: 3058 chosen: 500\n",
      "total: 1750 chosen: 500\n",
      "total: 3032 chosen: 500\n",
      "total: 804 chosen: 500\n",
      "total: 3042 chosen: 500\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(subject_file)\n",
    "root_path = os.path.abspath(os.getcwd())\n",
    "subjects = []\n",
    "for _, subject in df.iterrows():\n",
    "    paths = get_image_list(root_path, subject[1], subject[2])\n",
    "    subjects.append(paths)\n",
    "\n",
    "\n",
    "# Right now we only have 12 subjects! So we split them manually. \n",
    "\n",
    "split_index = 9\n",
    "train_all, test_all = subjects[:split_index], subjects[split_index:]\n",
    "train = [element for nestedlist in train_all for element in nestedlist]\n",
    "valid = [element for nestedlist in test_all for element in nestedlist[:150]]\n",
    "test = [element for nestedlist in test_all for element in nestedlist[150:]]\n",
    "\n",
    "data_transform = transforms.Compose([transforms.ToTensor(), transforms.Resize(size=(512, 512)),\n",
    "                                     transforms.Normalize(mean=(2**(BITS-1) - 0.5), std=(2**(BITS-1)))])\n",
    "\n",
    "train_dataset = DicomData(train, transform=data_transform)\n",
    "valid_dataset = DicomData(valid, transform=data_transform)\n",
    "test_dataset = DicomData(test, transform=data_transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d580dc9d-17f5-4ef3-89bd-1d8298ef84d8",
   "metadata": {},
   "source": [
    "<font size=\"5\">Models modification and custom loss function</font>\n",
    "\n",
    "The first layer of models needs to change because medical images have 1 channel, compared to RGB which have 3.\n",
    "<br>\n",
    "The classifier layer needs to be changed to match the number of classes.\n",
    "<br>\n",
    "Since classes are imbalanced, we test focal loss alongside cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9130de6d-3c59-49e5-8201-cc0675d85009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first layer of models needs to change because medical images have 1 channel, compared to RGB which have 3.\n",
    "# The classifier layer needs to be changed to match the number of classes.\n",
    "\n",
    "def buildResNet18Model(numClasses, dropout=0):\n",
    "    \n",
    "    model = models.resnet18()\n",
    "    model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    numFcInputs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(nn.Dropout(dropout),\n",
    "                             nn.Linear(numFcInputs, numClasses))\n",
    "    return model\n",
    "\n",
    "\n",
    "def buildEfficientNetModel(numClasses, dropout=0):\n",
    "    model = models.efficientnet_b0()\n",
    "    model.features[0] = nn.Sequential(nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n",
    "                 nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                           nn.SiLU(inplace=True))\n",
    "    numFcInputs = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(nn.Dropout(dropout),\n",
    "                             nn.Linear(numFcInputs, numClasses))\n",
    "    return model\n",
    "\n",
    "\n",
    "# Since classes are imbalanced, we test focal loss alongside cross-entropy\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=1, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "    \n",
    "        CE_loss = nn.CrossEntropyLoss()(inputs, targets)\n",
    "\n",
    "        pt = torch.exp(-CE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * CE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a91ce5f-0a9d-4348-b84c-96c1a70d000d",
   "metadata": {},
   "source": [
    "<font size=\"5\">Training loop and evaluation</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed86de51-c5f2-4ce0-96a3-b58972fd00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, epochs):\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for i, (images, labels) in enumerate(train_loader):  \n",
    "            # Move tensors to the configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            del images, labels, outputs\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "        return total_loss/len(train_loader)\n",
    "    \n",
    "        # print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "        #                .format(epoch+1, epochs, running_loss/len(train_loader)))\n",
    "\n",
    "def evaluate(model, valid_loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    valid_loss = 0.0\n",
    "    for images, labels in valid_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        valid_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    validation_accuracy = 100 * correct / total\n",
    "    validation_loss = valid_loss / total\n",
    "    \n",
    "    # print('validation accuracy: {:.4f} %'.format(validation_accuracy))\n",
    "    # print('validation loss : {:.4f}'.format(validation_loss))\n",
    "    \n",
    "    return validation_accuracy, validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c06522-6eb2-4e20-92e2-9519a40facf3",
   "metadata": {},
   "source": [
    "<font size=\"5\">Optimizing hyperparameters using optuna</font>\n",
    "\n",
    "Mlflow is used for experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "966e2886-37d7-4cca-8fc6-598a9e029284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters(trial):\n",
    "    hyperparameters = {\n",
    "        'model_type': trial.suggest_categorical('model_type', ['ResNet', 'EfficientNet']),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True),\n",
    "        'loss_function': trial.suggest_categorical('loss_function', ['CE', 'FL']),\n",
    "        'dropout': trial.suggest_float('dropout', 0, 0.8, step=0.1),\n",
    "        'optimizer': trial.suggest_categorical('optimizer', ['Adam', 'SGD']),\n",
    "    }\n",
    "    return hyperparameters\n",
    "\n",
    "def objective(trial, optimization=True):\n",
    "    run_name = 'opt'+ str(trial.number) if optimization else 'train'\n",
    "    \n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        hyperparameters = get_hyperparameters(trial)\n",
    "        mlflow.log_params(hyperparameters)\n",
    "        \n",
    "        if hyperparameters['model_type'] == 'ResNet':\n",
    "            model = buildResNet18Model(NUM_CLASSES, dropout=hyperparameters['dropout'])\n",
    "        else:\n",
    "            model = buildEfficientNetModel(NUM_CLASSES, dropout=hyperparameters['dropout'])\n",
    "    \n",
    "        model = model.to(device)\n",
    "        if hyperparameters['loss_function'] == 'CE':\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            criterion = FocalLoss()\n",
    "        \n",
    "        if hyperparameters['optimizer'] == 'Adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=hyperparameters['learning_rate'])\n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=hyperparameters['learning_rate'])\n",
    "\n",
    "        best_accuracy = -float('inf')\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        epochs = OPT_EPOCHS if optimization else NUM_EPOCHS\n",
    "        for i in range(epochs):\n",
    "            train_loss = train(model, train_loader, optimizer, criterion, 1)\n",
    "\n",
    "            \n",
    "            val_accuracy, val_loss = evaluate(model, valid_loader, criterion)\n",
    "            if optimization is False:\n",
    "                print('Epoch [{}/{}]'.format(i+1, epochs))\n",
    "                print('Train loss: {:.4f}'.format(train_loss))\n",
    "                print('validation accuracy: {:.4f} %'.format(val_accuracy))\n",
    "                print('validation loss : {:.4f}'.format(val_loss))\n",
    "            \n",
    "            mlflow.log_metric('val_accuracy', val_accuracy, step=i)\n",
    "            mlflow.log_metric('val_loss', val_loss, step=i)\n",
    "            \n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "                best_loss = val_loss # This is the loss of the best accuracy, not the best loss!\n",
    "\n",
    "                # We don't need to save model in optimization runs!\n",
    "                if optimization is False:\n",
    "                    mlflow.pytorch.log_model(model, 'models/best')\n",
    "        \n",
    "        # Since we use two different losses, we can't optimize loss.\n",
    "        return best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7d03ac1-263b-403e-86a0-e9951d0ec8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'CAD-classification'\n",
    "try:\n",
    "    mlflow.create_experiment(experiment_name)\n",
    "except:\n",
    "    print('Experiment already exists')\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "study = optuna.create_study(study_name='CAD-classification', direction='maximize')\n",
    "study.optimize(objective, n_trials=OPT_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8607f70-76d8-49aa-bbab-6744dced6093",
   "metadata": {},
   "source": [
    "<font size=\"5\">Training with best hyperparameters</font>\n",
    "\n",
    "After finding the best hyperparameters, a model is trained and saved using Mlflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91b9d6a9-4b4c-475f-ae1b-c3c862f6d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "objective(study.best_trial, optimization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4868b71e-adc8-4fbd-bb9e-eb138eb49c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
